{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ed4ecb18-f341-488a-a03a-ccaf20fa76d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import urllib.request\n",
    "import zipfile\n",
    "\n",
    "url = \"https://download.pytorch.org/tutorial/hymenoptera_data.zip\"\n",
    "local_file = \"./data/hymenoptera_data.zip\"\n",
    "urllib.request.urlretrieve(url, local_file)\n",
    "\n",
    "with zipfile.ZipFile(local_file, \"r\") as zip_ref:\n",
    "    zip_ref.extractall(\"./data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "88644198-32a3-4bb7-8ed4-3b1c4fec7efa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import albumentations as A\n",
    "import numpy as np\n",
    "import PIL.Image\n",
    "import torch\n",
    "import torchvision\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "\n",
    "class AntsBeesDataset(torchvision.datasets.ImageFolder):\n",
    "    def __getitem__(self, index: int):\n",
    "        \"\"\"overrides __getitem__ to be compatible to albumentations\"\"\"\n",
    "        path, target = self.samples[index]\n",
    "        sample = self.loader(path)\n",
    "        sample = self.get_cv2_image(sample)\n",
    "        if self.transforms is not None:\n",
    "            transformed = self.transforms(image=sample, target=target)\n",
    "            sample, target = transformed[\"image\"], transformed[\"target\"]\n",
    "        else:\n",
    "            if self.transform is not None:\n",
    "                sample = self.transform(image=sample)[\"image\"]\n",
    "            if self.target_transform is not None:\n",
    "                target = self.target_transform(target)\n",
    "\n",
    "        return sample, target\n",
    "\n",
    "    def get_cv2_image(self, image):\n",
    "        if isinstance(image, PIL.Image.Image):\n",
    "            return np.array(image).astype(\"uint8\")\n",
    "        elif isinstance(image, np.ndarray):\n",
    "            return image\n",
    "        else:\n",
    "            raise RuntimeError(\"Only PIL.Image and CV2 loaders currently supported!\")\n",
    "\n",
    "\n",
    "data_dir = \"./data/hymenoptera_data\"\n",
    "# Just normalization for validation\n",
    "data_transforms = A.Compose(\n",
    "    [\n",
    "        A.Resize(height=256, width=256),\n",
    "        A.CenterCrop(height=224, width=224),\n",
    "        A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n",
    "        ToTensorV2(),\n",
    "    ]\n",
    ")\n",
    "train_dataset = AntsBeesDataset(root=os.path.join(data_dir, \"train\"))\n",
    "train_dataset.transforms = data_transforms\n",
    "\n",
    "test_dataset = AntsBeesDataset(root=os.path.join(data_dir, \"val\"))\n",
    "test_dataset.transforms = data_transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "88edbe30-53df-4d22-a71b-688035839a3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training images: 244\n",
      "Number of validation images: 153\n",
      "Example output of an image shape: torch.Size([3, 224, 224])\n",
      "Example output of a label: 0\n"
     ]
    }
   ],
   "source": [
    "print(f\"Number of training images: {len(train_dataset)}\")\n",
    "print(f\"Number of validation images: {len(test_dataset)}\")\n",
    "print(f\"Example output of an image shape: {train_dataset[0][0].shape}\")\n",
    "print(f\"Example output of a label: {train_dataset[0][1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "590145a9-e26a-4c59-891a-345bd2f7ef81",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniconda/base/envs/ml-playground/lib/python3.9/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/opt/homebrew/Caskroom/miniconda/base/envs/ml-playground/lib/python3.9/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model = torchvision.models.resnet18(pretrained=True)\n",
    "num_ftrs = model.fc.in_features\n",
    "# We have only 2 classes\n",
    "model.fc = nn.Linear(num_ftrs, 2)\n",
    "model = model.to(device)\n",
    "_ = model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7e7bbd1b-6e36-42ca-8a64-084a98ac8ba1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from deepchecks.vision.vision_data import BatchOutputFormat\n",
    "\n",
    "\n",
    "def deepchecks_collate_fn(batch) -> BatchOutputFormat:\n",
    "    \"\"\"Return a batch of images, labels and predictions for a batch of data. The expected format is a dictionary with\n",
    "    the following keys: 'images', 'labels' and 'predictions', each value is in the deepchecks format for the task.\n",
    "    You can also use the BatchOutputFormat class to create the output.\n",
    "    \"\"\"\n",
    "    # batch received as iterable of tuples of (image, label) and transformed to tuple of iterables of images and labels:\n",
    "    batch = tuple(zip(*batch))\n",
    "\n",
    "    # images:\n",
    "    inp = torch.stack(batch[0]).detach().numpy().transpose((0, 2, 3, 1))\n",
    "    mean = [0.485, 0.456, 0.406]\n",
    "    std = [0.229, 0.224, 0.225]\n",
    "    inp = std * inp + mean\n",
    "    images = np.clip(inp, 0, 1) * 255\n",
    "\n",
    "    # labels:\n",
    "    labels = batch[1]\n",
    "\n",
    "    # predictions:\n",
    "    logits = model.to(device)(torch.stack(batch[0]).to(device))\n",
    "    predictions = nn.Softmax(dim=1)(logits)\n",
    "    return BatchOutputFormat(images=images, labels=labels, predictions=predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "711121ea-e72f-4068-9015-5c897df9fd96",
   "metadata": {},
   "outputs": [],
   "source": [
    "LABEL_MAP = {0: \"ants\", 1: \"bees\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7aca3093-d60b-4991-a8ba-da349de3fdcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from deepchecks.vision import VisionData\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_dataset, batch_size=4, shuffle=True, collate_fn=deepchecks_collate_fn\n",
    ")\n",
    "test_loader = DataLoader(\n",
    "    test_dataset, batch_size=4, shuffle=True, collate_fn=deepchecks_collate_fn\n",
    ")\n",
    "\n",
    "training_data = VisionData(\n",
    "    batch_loader=train_loader, task_type=\"classification\", label_map=LABEL_MAP\n",
    ")\n",
    "test_data = VisionData(\n",
    "    batch_loader=test_loader, task_type=\"classification\", label_map=LABEL_MAP\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "117292c8-6d11-4bbb-a351-d896609ca393",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ed3cbf64ff49426296d604b6d25c0e98",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<div style=\"display:flex; flex-direction: column; gap: 10px;\">\\n                <diâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "training_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6e64f60d-803f-4ab8-8910-d335f748e5fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <style>\n",
       "        progress {\n",
       "            -webkit-appearance: none;\n",
       "            border: none;\n",
       "            border-radius: 3px;\n",
       "            width: 300px;\n",
       "            height: 20px;\n",
       "            vertical-align: middle;\n",
       "            margin-right: 10px;\n",
       "            background-color: aliceblue;\n",
       "        }\n",
       "        progress::-webkit-progress-bar {\n",
       "            border-radius: 3px;\n",
       "            background-color: aliceblue;\n",
       "        }\n",
       "        progress::-webkit-progress-value {\n",
       "            background-color: #9d60fb;\n",
       "        }\n",
       "        progress::-moz-progress-bar {\n",
       "            background-color: #9d60fb;\n",
       "        }\n",
       "    </style>\n",
       "    "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <style>\n",
       "        progress {\n",
       "            -webkit-appearance: none;\n",
       "            border: none;\n",
       "            border-radius: 3px;\n",
       "            width: 300px;\n",
       "            height: 20px;\n",
       "            vertical-align: middle;\n",
       "            margin-right: 10px;\n",
       "            background-color: aliceblue;\n",
       "        }\n",
       "        progress::-webkit-progress-bar {\n",
       "            border-radius: 3px;\n",
       "            background-color: aliceblue;\n",
       "        }\n",
       "        progress::-webkit-progress-value {\n",
       "            background-color: #9d60fb;\n",
       "        }\n",
       "        progress::-moz-progress-bar {\n",
       "            background-color: #9d60fb;\n",
       "        }\n",
       "    </style>\n",
       "    "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <style>\n",
       "        progress {\n",
       "            -webkit-appearance: none;\n",
       "            border: none;\n",
       "            border-radius: 3px;\n",
       "            width: 300px;\n",
       "            height: 20px;\n",
       "            vertical-align: middle;\n",
       "            margin-right: 10px;\n",
       "            background-color: aliceblue;\n",
       "        }\n",
       "        progress::-webkit-progress-bar {\n",
       "            border-radius: 3px;\n",
       "            background-color: aliceblue;\n",
       "        }\n",
       "        progress::-webkit-progress-value {\n",
       "            background-color: #9d60fb;\n",
       "        }\n",
       "        progress::-moz-progress-bar {\n",
       "            background-color: #9d60fb;\n",
       "        }\n",
       "    </style>\n",
       "    "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <style>\n",
       "        progress {\n",
       "            -webkit-appearance: none;\n",
       "            border: none;\n",
       "            border-radius: 3px;\n",
       "            width: 300px;\n",
       "            height: 20px;\n",
       "            vertical-align: middle;\n",
       "            margin-right: 10px;\n",
       "            background-color: aliceblue;\n",
       "        }\n",
       "        progress::-webkit-progress-bar {\n",
       "            border-radius: 3px;\n",
       "            background-color: aliceblue;\n",
       "        }\n",
       "        progress::-webkit-progress-value {\n",
       "            background-color: #9d60fb;\n",
       "        }\n",
       "        progress::-moz-progress-bar {\n",
       "            background-color: #9d60fb;\n",
       "        }\n",
       "    </style>\n",
       "    "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <style>\n",
       "        progress {\n",
       "            -webkit-appearance: none;\n",
       "            border: none;\n",
       "            border-radius: 3px;\n",
       "            width: 300px;\n",
       "            height: 20px;\n",
       "            vertical-align: middle;\n",
       "            margin-right: 10px;\n",
       "            background-color: aliceblue;\n",
       "        }\n",
       "        progress::-webkit-progress-bar {\n",
       "            border-radius: 3px;\n",
       "            background-color: aliceblue;\n",
       "        }\n",
       "        progress::-webkit-progress-value {\n",
       "            background-color: #9d60fb;\n",
       "        }\n",
       "        progress::-moz-progress-bar {\n",
       "            background-color: #9d60fb;\n",
       "        }\n",
       "    </style>\n",
       "    "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": []
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from deepchecks.vision.suites import train_test_validation\n",
    "\n",
    "suite = train_test_validation()\n",
    "result = suite.run(training_data, test_data, max_samples=5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9ddfd626-7eec-4dc8-9313-c5d23e3d73ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <style>\n",
       "        progress {\n",
       "            -webkit-appearance: none;\n",
       "            border: none;\n",
       "            border-radius: 3px;\n",
       "            width: 300px;\n",
       "            height: 20px;\n",
       "            vertical-align: middle;\n",
       "            margin-right: 10px;\n",
       "            background-color: aliceblue;\n",
       "        }\n",
       "        progress::-webkit-progress-bar {\n",
       "            border-radius: 3px;\n",
       "            background-color: aliceblue;\n",
       "        }\n",
       "        progress::-webkit-progress-value {\n",
       "            background-color: #9d60fb;\n",
       "        }\n",
       "        progress::-moz-progress-bar {\n",
       "            background-color: #9d60fb;\n",
       "        }\n",
       "    </style>\n",
       "    "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <style>\n",
       "        progress {\n",
       "            -webkit-appearance: none;\n",
       "            border: none;\n",
       "            border-radius: 3px;\n",
       "            width: 300px;\n",
       "            height: 20px;\n",
       "            vertical-align: middle;\n",
       "            margin-right: 10px;\n",
       "            background-color: aliceblue;\n",
       "        }\n",
       "        progress::-webkit-progress-bar {\n",
       "            border-radius: 3px;\n",
       "            background-color: aliceblue;\n",
       "        }\n",
       "        progress::-webkit-progress-value {\n",
       "            background-color: #9d60fb;\n",
       "        }\n",
       "        progress::-moz-progress-bar {\n",
       "            background-color: #9d60fb;\n",
       "        }\n",
       "    </style>\n",
       "    "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <style>\n",
       "        progress {\n",
       "            -webkit-appearance: none;\n",
       "            border: none;\n",
       "            border-radius: 3px;\n",
       "            width: 300px;\n",
       "            height: 20px;\n",
       "            vertical-align: middle;\n",
       "            margin-right: 10px;\n",
       "            background-color: aliceblue;\n",
       "        }\n",
       "        progress::-webkit-progress-bar {\n",
       "            border-radius: 3px;\n",
       "            background-color: aliceblue;\n",
       "        }\n",
       "        progress::-webkit-progress-value {\n",
       "            background-color: #9d60fb;\n",
       "        }\n",
       "        progress::-moz-progress-bar {\n",
       "            background-color: #9d60fb;\n",
       "        }\n",
       "    </style>\n",
       "    "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <style>\n",
       "        progress {\n",
       "            -webkit-appearance: none;\n",
       "            border: none;\n",
       "            border-radius: 3px;\n",
       "            width: 300px;\n",
       "            height: 20px;\n",
       "            vertical-align: middle;\n",
       "            margin-right: 10px;\n",
       "            background-color: aliceblue;\n",
       "        }\n",
       "        progress::-webkit-progress-bar {\n",
       "            border-radius: 3px;\n",
       "            background-color: aliceblue;\n",
       "        }\n",
       "        progress::-webkit-progress-value {\n",
       "            background-color: #9d60fb;\n",
       "        }\n",
       "        progress::-moz-progress-bar {\n",
       "            background-color: #9d60fb;\n",
       "        }\n",
       "    </style>\n",
       "    "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <style>\n",
       "        progress {\n",
       "            -webkit-appearance: none;\n",
       "            border: none;\n",
       "            border-radius: 3px;\n",
       "            width: 300px;\n",
       "            height: 20px;\n",
       "            vertical-align: middle;\n",
       "            margin-right: 10px;\n",
       "            background-color: aliceblue;\n",
       "        }\n",
       "        progress::-webkit-progress-bar {\n",
       "            border-radius: 3px;\n",
       "            background-color: aliceblue;\n",
       "        }\n",
       "        progress::-webkit-progress-value {\n",
       "            background-color: #9d60fb;\n",
       "        }\n",
       "        progress::-moz-progress-bar {\n",
       "            background-color: #9d60fb;\n",
       "        }\n",
       "    </style>\n",
       "    "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": []
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from deepchecks.vision.suites import train_test_validation\n",
    "\n",
    "suite = train_test_validation()\n",
    "result = suite.run(training_data, test_data, max_samples=5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "59335913-3eb1-43ba-8ee6-dcff9427bbc5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'deepchecks-output (1).html'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.save_as_html(\"deepchecks-output.html\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87dd1cb7-a6e6-49d2-b35e-ed694db4fcfe",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
